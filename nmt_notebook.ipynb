{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45379396-8b29-4153-8ab3-15c2c34c1a7f",
   "metadata": {},
   "source": [
    "<h1> Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049035b1-3d64-47b6-8c11-35324fadc5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using hugging face translation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b1ef5-f106-4b78-83cf-7e0cb766808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate sacrebleu\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install sentencepiece\n",
    "!pip install sacremoses\n",
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4505c563-b11e-494c-9ae9-6f36fc3ac768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\smuts\\Documents\\chcaa\\compling_2025\\neural_machine_translation_lesson\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import evaluate\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b49a685-c4c3-4eae-97eb-5b10370b9a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say you want to translate a sentence from English to French. Try running the default model T5-base for your translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0bdc2-e7c9-4ef3-9972-5918b7e915b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_fr_translator = pipeline(\"translation_en_to_fr\")\n",
    "en_fr_translator(\"How old are you?\")\n",
    "## [{'translation_text': ' quel âge êtes-vous?'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d71916a2-11be-417f-b281-c7c9ce11c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, try loading a specific model for your languages. For more models, check the webpage: https://huggingface.co/Helsinki-NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32f03d-5331-424c-ad2e-6756754fa9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9510b814-e0f8-4ad4-9bbd-f4ca296a246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your own corpus and explore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3e13fc8-2d07-49eb-a457-7be383e8dbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_corpus = [ \"This is a test.\", \"This is another test.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ee7ea9f-3d78-431f-b877-590e9c2d026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a function that takes your corpus as input and returns their translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f27d88e4-383f-4dba-978e-5ef387799d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentences(my_corpus):\n",
    "    translations = []  \n",
    "    for sentence in my_corpus:\n",
    "        translated_text = translator(sentence)[0]['translation_text'] \n",
    "        translations.append(translated_text)  \n",
    "    return translations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d87d0fd-cdf7-4686-86a8-b434ca5421b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentences = translate_sentences(my_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3780d5d0-c84c-44b0-8239-aec6eb272f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8678ccb1-ba0a-407c-954d-9cf6a6739bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to evaluate your translations, you are going to need a reference translations (the correct ones). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc278434-1e2f-4934-a225-ae5bdfb59f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the translation using the Bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e66b8b-686d-4393-b60b-c253bbc3d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c1d574b-9464-4709-9a71-77ba1b1aed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_translations = [\"C'est un test.\", \"C'est un autre test.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b268c27-9bf1-48f9-9d8e-a043645820db",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_translation = target_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00c3cfc4-82b8-4ac9-a398-559b590846a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score = metric.compute(predictions=predicted_translation, references=reference_translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdda532-2fc6-4b4f-8f62-526aa8ed476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BLEU Score:\", bleu_score[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-explanation",
   "metadata": {},
   "source": [
    "## Understanding Translation Evaluation Metrics: SACREBLEU and BERTScore\n",
    "\n",
    "Translation quality metrics help us evaluate how good a machine translation is compared to a human reference. Let's understand the two main metrics we're using:\n",
    "\n",
    "### SACREBLEU: The Traditional Word-Matching Approach\n",
    "\n",
    "SACREBLEU is a standardized implementation of BLEU (Bilingual Evaluation Understudy), which has been the industry standard for years.\n",
    "\n",
    "#### How SACREBLEU Works (with Analogies)\n",
    "\n",
    "1. **N-gram Matching**: SACREBLEU counts how many word sequences in your translation match those in the reference translation.\n",
    "   \n",
    "   **Analogy**: Imagine comparing two recipes. SACREBLEU checks how many exact ingredient combinations (like \"salt and pepper\" or \"olive oil and garlic\") appear in both recipes.\n",
    "\n",
    "2. **Brevity Penalty**: It penalizes translations that are too short.\n",
    "   \n",
    "   **Analogy**: Like a teacher deducting points if your essay is only half a page when it should be two pages, even if what you wrote was correct.\n",
    "\n",
    "3. **Tokenization**: SACREBLEU uses standardized tokenization methods to ensure consistency.\n",
    "   \n",
    "   **Analogy**: Before comparing two documents, making sure both use the same formatting rules (like how to handle punctuation).\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- **Surface-level Matching**: It only recognizes exact matches without understanding meaning.\n",
    "  \n",
    "  **Analogy**: Judging two chefs solely on whether they used identical ingredients, not on whether their dishes taste the same. \"Quick brown dog\" and \"fast auburn canine\" would be considered completely different.\n",
    "\n",
    "- **Word Order Sensitivity**: SACREBLEU can be overly sensitive to word order changes.\n",
    "  \n",
    "  **Analogy**: Considering \"I love pasta with tomato sauce\" and \"I love tomato sauce with pasta\" as significantly different statements.\n",
    "\n",
    "### BERTScore: The Semantic Understanding Approach\n",
    "\n",
    "BERTScore evaluates translations based on semantic similarity rather than exact word matches.\n",
    "\n",
    "#### How BERTScore Works (with Analogies)\n",
    "\n",
    "1. **Contextual Embeddings**: BERTScore converts words into vector representations that capture their meaning in context.\n",
    "   \n",
    "   **Analogy**: Understanding that \"automobile\" and \"car\" refer to the same thing, or that \"bank\" means something different in \"river bank\" versus \"money in the bank.\"\n",
    "\n",
    "2. **Token Matching**: It matches words based on semantic similarity.\n",
    "   \n",
    "   **Analogy**: Matching ingredients based on their culinary function rather than exact names - recognizing that \"cayenne\" could be a suitable substitute for \"chili powder\" in recipes.\n",
    "\n",
    "3. **Precision, Recall, and F1**: BERTScore calculates these metrics to produce a final score.\n",
    "   \n",
    "   **Analogy**: Evaluating a conversation by checking: did you cover all important points (recall)? Did you avoid irrelevant things (precision)? And how well did you balance both (F1 score)?\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- **Semantic Understanding**: BERTScore recognizes synonyms and paraphrases.\n",
    "  \n",
    "  **Analogy**: A music critic who can recognize when two different arrangements convey the same melody, even if using different instruments.\n",
    "\n",
    "- **Contextual Awareness**: It understands how words' meanings change based on context.\n",
    "  \n",
    "  **Analogy**: Understanding that \"cool\" means something different in \"the weather is cool\" versus \"that's a cool gadget.\"\n",
    "\n",
    "### Practical Example\n",
    "\n",
    "Reference: \"The cat quickly ran across the park.\"  \n",
    "Translation 1: \"The feline rapidly crossed the park.\"  \n",
    "Translation 2: \"A cat park the across quickly ran.\"\n",
    "\n",
    "**SACREBLEU Evaluation**:\n",
    "- Translation 1 would get a low score because few exact words match\n",
    "- Translation 2 might get a similar score because it has the same words, despite being nonsensical\n",
    "\n",
    "**BERTScore Evaluation**:\n",
    "- Translation 1 would get a high score because semantically it's very similar\n",
    "- Translation 2 would get a low score because semantically it makes no sense\n",
    "\n",
    "This is why using both metrics gives us a more complete picture of translation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d91e5a42-a77a-4746-87c2-4cedd0b3cd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the translation using the BERT score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2e03a0e-9e11-468a-9872-d84de6ab774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedbbc4d-0593-4edd-a495-eca1c1c3e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = bertscore.compute(predictions=target_sentences, references=reference_translations, lang=\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e5f9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8828ab30-c060-4139-b270-0e62b8ca17de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access f1 scores directly from results dictionary\n",
    "average_f1_score = sum(results['f1']) / len(results['f1'])  # Calculate the average F1 score\n",
    "print(\"BERTScore F1:\", average_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4344207-0352-4349-a50a-9933a3481e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare a single sentence using Bleu and BERT. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-comparison",
   "metadata": {},
   "source": [
    "## Practical Comparison: BERTScore vs SACREBLEU\n",
    "\n",
    "Let's demonstrate the difference between these metrics using our example sentences from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "practical-example-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our reference and two translation examples\n",
    "reference = [\"The cat quickly ran across the park.\"]\n",
    "translation1 = [\"The feline rapidly crossed the park.\"]  # Semantically similar but different words\n",
    "translation2 = [\"A cat park the across quickly ran.\"]    # Same words but nonsensical order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bleu-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the SACREBLEU metric\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "# Calculate BLEU scores for both translations\n",
    "bleu_score1 = bleu_metric.compute(predictions=translation1, references=[reference])\n",
    "bleu_score2 = bleu_metric.compute(predictions=translation2, references=[reference])\n",
    "\n",
    "print(\"SACREBLEU Scores:\")\n",
    "print(f\"Translation 1 (semantically similar): {bleu_score1['score']:.2f}\")\n",
    "print(f\"Translation 2 (nonsensical order): {bleu_score2['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bert-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the BERTScore metric\n",
    "bert_metric = load(\"bertscore\")\n",
    "\n",
    "# Calculate BERTScores for both translations\n",
    "bert_results1 = bert_metric.compute(predictions=translation1, references=reference, lang=\"en\")\n",
    "bert_results2 = bert_metric.compute(predictions=translation2, references=reference, lang=\"en\")\n",
    "\n",
    "print(\"BERTScore F1 Scores:\")\n",
    "print(f\"Translation 1 (semantically similar): {bert_results1['f1'][0]:.4f}\")\n",
    "print(f\"Translation 2 (nonsensical order): {bert_results2['f1'][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Normalize BLEU scores to 0-1 range for comparison\n",
    "bleu1_norm = bleu_score1['score'] / 100\n",
    "bleu2_norm = bleu_score2['score'] / 100\n",
    "\n",
    "# Set up the comparison data\n",
    "translations = ['Semantically Similar', 'Nonsensical Order']\n",
    "bleu_scores = [bleu1_norm, bleu2_norm]\n",
    "bert_scores = [bert_results1['f1'][0], bert_results2['f1'][0]]\n",
    "\n",
    "# Set width of bars\n",
    "barWidth = 0.3\n",
    "r1 = np.arange(len(translations))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "\n",
    "# Create the bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(r1, bleu_scores, width=barWidth, label='SACREBLEU (normalized)')\n",
    "plt.bar(r2, bert_scores, width=barWidth, label='BERTScore F1')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Translation Type')\n",
    "plt.ylabel('Score (0-1 scale)')\n",
    "plt.title('SACREBLEU vs BERTScore Comparison')\n",
    "plt.xticks([r + barWidth/2 for r in range(len(translations))], translations)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis",
   "metadata": {},
   "source": [
    "### Analysis of the Results\n",
    "\n",
    "The comparison demonstrates the key differences between SACREBLEU and BERTScore:\n",
    "\n",
    "1. **For semantically similar but lexically different translations (Translation 1)**:\n",
    "   - SACREBLEU gives a lower score (22.77) because it only counts exact matching words (\"the\" and \"park\" match, but \"feline\" != \"cat\", \"rapidly\" != \"quickly\", etc.)\n",
    "   - BERTScore gives a high score (0.9663) because it recognizes that \"feline\" is semantically similar to \"cat\" and \"rapidly\" is semantically similar to \"quickly\"\n",
    "\n",
    "2. **For nonsensical ordering of the same words (Translation 2)**:\n",
    "   - SACREBLEU gives a moderate score (15.11) because many individual words match, even though the meaning is lost\n",
    "   - BERTScore still gives a surprisingly high score (0.9228) because:\n",
    "     - It contains all the same words as the reference (shared vocabulary)\n",
    "     - BERT's contextual window has limitations in understanding complete grammar\n",
    "     - The averaging effect of token-level similarities boosts the score\n",
    "     - Transformer models like BERT capture some but not all word order information\n",
    "\n",
    "The important insight is the relative difference between the scores. BERTScore shows a more meaningful gap (0.9663 vs. 0.9228) between good and bad translations than SACREBLEU does proportionally.\n",
    "\n",
    "This demonstrates why using both metrics together gives us a more complete picture of translation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-example",
   "metadata": {},
   "source": [
    "## French Example: Applying Both Metrics\n",
    "\n",
    "Since we've been working with French translations in this notebook, let's try a similar experiment using French sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "french-example-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our French reference and two translation examples\n",
    "fr_reference = [\"Le chat a rapidement traversé le parc.\"]  # The cat quickly crossed the park\n",
    "fr_translation1 = [\"Le félin a vite traversé le parc.\"]    # The feline quickly crossed the park\n",
    "fr_translation2 = [\"Un chat parc le traversé rapidement.\"] # Nonsensical word order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-bleu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SACREBLEU scores for both French translations\n",
    "fr_bleu_score1 = bleu_metric.compute(predictions=fr_translation1, references=[fr_reference])\n",
    "fr_bleu_score2 = bleu_metric.compute(predictions=fr_translation2, references=[fr_reference])\n",
    "\n",
    "print(\"SACREBLEU Scores for French:\")\n",
    "print(f\"Translation 1 (semantically similar): {fr_bleu_score1['score']:.2f}\")\n",
    "print(f\"Translation 2 (nonsensical order): {fr_bleu_score2['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-bert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BERTScores for both French translations\n",
    "fr_bert_results1 = bert_metric.compute(predictions=fr_translation1, references=fr_reference, lang=\"fr\")\n",
    "fr_bert_results2 = bert_metric.compute(predictions=fr_translation2, references=fr_reference, lang=\"fr\")\n",
    "\n",
    "print(\"BERTScore F1 Scores for French:\")\n",
    "print(f\"Translation 1 (semantically similar): {fr_bert_results1['f1'][0]:.4f}\")\n",
    "print(f\"Translation 2 (nonsensical order): {fr_bert_results2['f1'][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the French example results\n",
    "# Normalize BLEU scores to 0-1 range for comparison\n",
    "fr_bleu1_norm = fr_bleu_score1['score'] / 100\n",
    "fr_bleu2_norm = fr_bleu_score2['score'] / 100\n",
    "\n",
    "# Set up the comparison data\n",
    "fr_translations = ['Sémantiquement Similaire', 'Ordre Incohérent']\n",
    "fr_bleu_scores = [fr_bleu1_norm, fr_bleu2_norm]\n",
    "fr_bert_scores = [fr_bert_results1['f1'][0], fr_bert_results2['f1'][0]]\n",
    "\n",
    "# Set width of bars\n",
    "barWidth = 0.3\n",
    "r1 = np.arange(len(fr_translations))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "\n",
    "# Create the bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(r1, fr_bleu_scores, width=barWidth, label='SACREBLEU (normalisé)')\n",
    "plt.bar(r2, fr_bert_scores, width=barWidth, label='BERTScore F1')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Type de Traduction')\n",
    "plt.ylabel('Score (échelle 0-1)')\n",
    "plt.title('Comparaison SACREBLEU vs BERTScore en Français')\n",
    "plt.xticks([r + barWidth/2 for r in range(len(fr_translations))], fr_translations)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-analysis",
   "metadata": {},
   "source": [
    "### Analysis of French Results\n",
    "\n",
    "The French example confirms our findings from the English comparison:\n",
    "\n",
    "1. **For semantically similar but lexically different translations**:\n",
    "   - SACREBLEU penalizes the use of \"félin\" instead of \"chat\" and \"vite\" instead of \"rapidement\", even though the meaning is preserved\n",
    "   - BERTScore maintains a high score because it recognizes the semantic similarity between these terms\n",
    "\n",
    "2. **For the nonsensical French sentence**:\n",
    "   - SACREBLEU still gives some credit because individual words match\n",
    "   - BERTScore also gives a relatively high score, but with a meaningful drop compared to the valid translation\n",
    "\n",
    "This pattern is consistent across languages because both metrics function the same way regardless of the language, though the exact scores may differ due to language-specific properties and the language capabilities of the underlying models.\n",
    "\n",
    "The French example reinforces that machine translation evaluation is most effective when using multiple complementary metrics together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-example",
   "metadata": {},
   "source": [
    "## French Example: Applying Both Metrics\n",
    "\n",
    "Since we've been working with French translations in this notebook, let's try a similar experiment using French sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "french-example-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our French reference and two translation examples\n",
    "fr_reference = [\"Le chat a rapidement traversé le parc.\"]  # The cat quickly crossed the park\n",
    "fr_translation1 = [\"Le félin a vite traversé le parc.\"]    # The feline quickly crossed the park\n",
    "fr_translation2 = [\"Un chat parc le traversé rapidement.\"] # Nonsensical word order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-bleu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SACREBLEU scores for both French translations\n",
    "fr_bleu_score1 = bleu_metric.compute(predictions=fr_translation1, references=[fr_reference])\n",
    "fr_bleu_score2 = bleu_metric.compute(predictions=fr_translation2, references=[fr_reference])\n",
    "\n",
    "print(\"SACREBLEU Scores for French:\")\n",
    "print(f\"Translation 1 (semantically similar): {fr_bleu_score1['score']:.2f}\")\n",
    "print(f\"Translation 2 (nonsensical order): {fr_bleu_score2['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-bert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BERTScores for both French translations\n",
    "fr_bert_results1 = bert_metric.compute(predictions=fr_translation1, references=fr_reference, lang=\"fr\")\n",
    "fr_bert_results2 = bert_metric.compute(predictions=fr_translation2, references=fr_reference, lang=\"fr\")\n",
    "\n",
    "print(\"BERTScore F1 Scores for French:\")\n",
    "print(f\"Translation 1 (semantically similar): {fr_bert_results1['f1'][0]:.4f}\")\n",
    "print(f\"Translation 2 (nonsensical order): {fr_bert_results2['f1'][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the French example results\n",
    "# Normalize BLEU scores to 0-1 range for comparison\n",
    "fr_bleu1_norm = fr_bleu_score1['score'] / 100\n",
    "fr_bleu2_norm = fr_bleu_score2['score'] / 100\n",
    "\n",
    "# Set up the comparison data\n",
    "fr_translations = ['Sémantiquement Similaire', 'Ordre Incohérent']\n",
    "fr_bleu_scores = [fr_bleu1_norm, fr_bleu2_norm]\n",
    "fr_bert_scores = [fr_bert_results1['f1'][0], fr_bert_results2['f1'][0]]\n",
    "\n",
    "# Set width of bars\n",
    "barWidth = 0.3\n",
    "r1 = np.arange(len(fr_translations))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "\n",
    "# Create the bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(r1, fr_bleu_scores, width=barWidth, label='SACREBLEU (normalisé)')\n",
    "plt.bar(r2, fr_bert_scores, width=barWidth, label='BERTScore F1')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Type de Traduction')\n",
    "plt.ylabel('Score (échelle 0-1)')\n",
    "plt.title('Comparaison SACREBLEU vs BERTScore en Français')\n",
    "plt.xticks([r + barWidth/2 for r in range(len(fr_translations))], fr_translations)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-analysis",
   "metadata": {},
   "source": [
    "### Analysis of French Results\n",
    "\n",
    "The French example confirms our findings from the English comparison:\n",
    "\n",
    "1. **For semantically similar but lexically different translations**:\n",
    "   - SACREBLEU penalizes the use of \"félin\" instead of \"chat\" and \"vite\" instead of \"rapidement\", even though the meaning is preserved\n",
    "   - BERTScore maintains a high score because it recognizes the semantic similarity between these terms\n",
    "\n",
    "2. **For the nonsensical French sentence**:\n",
    "   - SACREBLEU still gives some credit because individual words match\n",
    "   - BERTScore also gives a relatively high score, but with a meaningful drop compared to the valid translation\n",
    "\n",
    "This pattern is consistent across languages because both metrics function the same way regardless of the language, though the exact scores may differ due to language-specific properties and the language capabilities of the underlying models.\n",
    "\n",
    "The French example reinforces that machine translation evaluation is most effective when using multiple complementary metrics together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-english-example",
   "metadata": {},
   "source": [
    "## Cross-Language Example: French to English\n",
    "\n",
    "Since we've been working with French-English translation in this notebook, let's demonstrate how these metrics evaluate translations across languages. We'll use a French reference sentence and compare two English translations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "french-english-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our French reference and two English translation examples\n",
    "fr_reference = [\"Le chat a rapidement traversé le parc.\"]  # The cat quickly crossed the park\n",
    "en_translation1 = [\"The cat quickly ran across the park.\"]  # Semantically accurate translation\n",
    "en_translation2 = [\"The park across quickly cat the.\"]      # Nonsensical word order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-lang-bleu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a special setup for cross-language evaluation\n",
    "# First, we'll use our fr-en model to create a proper reference\n",
    "print(\"Creating a reference English translation using our model:\")\n",
    "model_reference = translator(fr_reference[0])[0]['translation_text']\n",
    "print(f\"Model reference: {model_reference}\")\n",
    "print(\"\\nNow comparing our two translations against this reference:\")\n",
    "\n",
    "# Calculate SACREBLEU scores for both translations against the model reference\n",
    "bleu_score_cross1 = bleu_metric.compute(predictions=en_translation1, references=[[model_reference]])\n",
    "bleu_score_cross2 = bleu_metric.compute(predictions=en_translation2, references=[[model_reference]])\n",
    "\n",
    "print(\"\\nSACREBLEU Scores (French to English):\")\n",
    "print(f\"Translation 1 (semantically accurate): {bleu_score_cross1['score']:.2f}\")\n",
    "print(f\"Translation 2 (nonsensical order): {bleu_score_cross2['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-lang-bert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BERTScores for our English translations against the model reference\n",
    "# We use the multilingual model to properly handle cross-language comparison\n",
    "bert_results_cross1 = bert_metric.compute(predictions=en_translation1, references=[model_reference], lang=\"en\")\n",
    "bert_results_cross2 = bert_metric.compute(predictions=en_translation2, references=[model_reference], lang=\"en\")\n",
    "\n",
    "print(\"BERTScore F1 Scores (French to English):\")\n",
    "print(f\"Translation 1 (semantically accurate): {bert_results_cross1['f1'][0]:.4f}\")\n",
    "print(f\"Translation 2 (nonsensical order): {bert_results_cross2['f1'][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-lang-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cross-language results\n",
    "# Normalize BLEU scores to 0-1 range for comparison\n",
    "bleu1_norm = bleu_score_cross1['score'] / 100\n",
    "bleu2_norm = bleu_score_cross2['score'] / 100\n",
    "\n",
    "# Set up the comparison data\n",
    "translations = ['Semantically Accurate', 'Nonsensical Order']\n",
    "bleu_scores = [bleu1_norm, bleu2_norm]\n",
    "bert_scores = [bert_results_cross1['f1'][0], bert_results_cross2['f1'][0]]\n",
    "\n",
    "# Set width of bars\n",
    "barWidth = 0.3\n",
    "r1 = np.arange(len(translations))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "\n",
    "# Create the bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(r1, bleu_scores, width=barWidth, label='SACREBLEU (normalized)')\n",
    "plt.bar(r2, bert_scores, width=barWidth, label='BERTScore F1')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Translation Type')\n",
    "plt.ylabel('Score (0-1 scale)')\n",
    "plt.title('Cross-Language Evaluation: French to English')\n",
    "plt.xticks([r + barWidth/2 for r in range(len(translations))], translations)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-lang-analysis",
   "metadata": {},
   "source": [
    "### Analysis of Cross-Language Results\n",
    "\n",
    "This cross-language example (French to English) demonstrates how translation metrics work when evaluating translations between different languages:\n",
    "\n",
    "1. **Cross-language challenges**:\n",
    "   - When evaluating translations between languages, we need reference translations in the target language\n",
    "   - Here we used our translation model to create an English reference from the French original\n",
    "   - This approach simulates real-world evaluation of machine translation systems\n",
    "\n",
    "2. **Metric behavior is consistent**:\n",
    "   - SACREBLEU still prioritizes exact n-gram matches\n",
    "   - BERTScore still captures semantic similarity\n",
    "   - The pattern of scores for good vs. bad translations is similar to our monolingual examples\n",
    "\n",
    "3. **Practical implications**:\n",
    "   - When developing translation systems, we evaluate using references in the target language\n",
    "   - Using multiple metrics gives a more complete picture of translation quality\n",
    "   - Context and intended use should guide which metric to prioritize\n",
    "\n",
    "This example highlights why neural machine translation often uses multiple evaluation metrics when assessing system performance across languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e1dc7-808c-473b-99f4-d7a576f19619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with translations of other languages you know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a6182-2daf-454b-b678-acd3c1e852ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the accuracy scores bidirectional with the languages?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
