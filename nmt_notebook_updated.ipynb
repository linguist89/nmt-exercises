{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13314067",
   "metadata": {},
   "source": [
    "# Neural Machine Translation: Model Comparison Across Topics\n",
    "\n",
    "In this notebook, we'll explore how different Neural Machine Translation (NMT) models perform across various topics. We'll learn how to:\n",
    "1. Load and prepare different types of translation data\n",
    "2. Use the Hugging Face pipeline for translation\n",
    "3. Evaluate translations using multiple metrics\n",
    "4. Visualize and analyze model performance\n",
    "5. Compare translations across different topics\n",
    "\n",
    "Let's start by setting up our environment and loading the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e19b937",
   "metadata": {},
   "source": [
    "First, we need to install and import the required libraries. We'll use:\n",
    "- `transformers` for the translation models\n",
    "- `pandas` for data handling\n",
    "- `matplotlib` and `seaborn` for visualization\n",
    "- `sacrebleu` and `bert_score` for evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f71d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install transformers datasets evaluate sacrebleu\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install sentencepiece\n",
    "!pip install sacremoses\n",
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa958ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import pipeline\n",
    "from bert_score import score\n",
    "from sacrebleu import corpus_bleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('default')  # Using default style instead of seaborn\n",
    "sns.set_theme()  # This will apply seaborn's styling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d50f581",
   "metadata": {},
   "source": [
    "## 1. Loading and Preparing Our Datasets\n",
    "\n",
    "We'll be working with three different datasets, each focusing on a specific topic:\n",
    "1. General translations (everyday conversations and situations)\n",
    "2. Political translations (government, policies, elections)\n",
    "3. Sports translations (matches, tournaments, athletes)\n",
    "\n",
    "Let's first create a function to load our CSV files and then combine them into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1148fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading general translations...\n",
      "Loading political translations...\n",
      "Loading sports translations...\n"
     ]
    }
   ],
   "source": [
    "# Function to load a dataset from a CSV file\n",
    "def load_dataset(file_path):\n",
    "    df = pd.read_csv(file_path,sep=\"\\t\")\n",
    "    return df\n",
    "\n",
    "# Load each dataset\n",
    "print(\"Loading general translations...\")\n",
    "general_df = load_dataset('data/general_translations.csv')\n",
    "\n",
    "print(\"Loading political translations...\")\n",
    "politics_df = load_dataset('data/political_translations.csv')\n",
    "\n",
    "print(\"Loading sports translations...\")\n",
    "sports_df = load_dataset('data/sports_translations.csv')\n",
    "\n",
    "# Add topic labels to each dataset\n",
    "general_df['topic'] = 'general'\n",
    "politics_df['topic'] = 'politics'\n",
    "sports_df['topic'] = 'sports'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db66f53",
   "metadata": {},
   "source": [
    "## 2. Setting Up Our Translation Models\n",
    "\n",
    "We'll compare three different translation models:\n",
    "1. T5-small: A smaller, faster version of the T5 model\n",
    "2. T5-base: The standard T5 model with better performance\n",
    "3. Helsinki-NLP: A specialized model for English to French translation\n",
    "\n",
    "Let's initialize these models using the Hugging Face pipeline. The pipeline makes it easy to use pre-trained models for various tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ef3f0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading T5-small...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading T5-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Helsinki-NLP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing each model with a simple sentence:\n",
      "\n",
      "T5-small:\n",
      "Input: Hello, how are you?\n",
      "Output: Bonjour, comment êtes-vous ?\n",
      "\n",
      "T5-base:\n",
      "Input: Hello, how are you?\n",
      "Output: Bonjour, comment êtes-vous?\n",
      "\n",
      "Helsinki-NLP:\n",
      "Input: Hello, how are you?\n",
      "Output: Bonjour, comment allez-vous ?\n"
     ]
    }
   ],
   "source": [
    "# Initialize translation pipelines\n",
    "print(\"Loading T5-small...\")\n",
    "t5_small = pipeline(\"translation_en_to_fr\", model=\"t5-small\")\n",
    "\n",
    "print(\"Loading T5-base...\")\n",
    "t5_base = pipeline(\"translation_en_to_fr\", model=\"t5-base\")\n",
    "\n",
    "print(\"Loading Helsinki-NLP model...\")\n",
    "helsinki = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "\n",
    "# Store models in a dictionary for easier access\n",
    "models = {\n",
    "    'T5-small': t5_small,\n",
    "    'T5-base': t5_base,\n",
    "    'Helsinki-NLP': helsinki\n",
    "}\n",
    "\n",
    "# Test each model with a simple sentence\n",
    "test_sentence = \"Hello, how are you?\"\n",
    "print(\"\\nTesting each model with a simple sentence:\")\n",
    "for model_name, model in models.items():\n",
    "    # Add task prefix for T5 models\n",
    "    if model_name.startswith('T5'):\n",
    "        text = f\"translate English to French: {test_sentence}\"\n",
    "    else:\n",
    "        text = test_sentence\n",
    "    \n",
    "    result = model(text)[0]['translation_text']\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"Input: {test_sentence}\")\n",
    "    print(f\"Output: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685c500",
   "metadata": {},
   "source": [
    "## 3. Translating Our Datasets\n",
    "\n",
    "Let's translate our datasets step by step. We'll:\n",
    "1. First translate a small subset to test our setup\n",
    "2. Then translate the full dataset for each topic\n",
    "3. Store the translations for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaf7e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Section 1: General Topic Translations ===\n",
      "This section will be completed individually.\n",
      "\n",
      "Translating general dataset...\n",
      "\n",
      "Translating with T5-small...\n",
      "\n",
      "Translating 50 texts with T5-small...\n",
      "Progress: 10/50 texts translated\n",
      "Progress: 20/50 texts translated\n",
      "Progress: 30/50 texts translated\n",
      "Progress: 40/50 texts translated\n",
      "Progress: 50/50 texts translated\n",
      "\n",
      "Translating with T5-base...\n",
      "\n",
      "Translating 50 texts with T5-base...\n",
      "Progress: 10/50 texts translated\n",
      "Progress: 20/50 texts translated\n",
      "Progress: 30/50 texts translated\n",
      "Progress: 40/50 texts translated\n",
      "Progress: 50/50 texts translated\n",
      "\n",
      "Translating with Helsinki-NLP...\n",
      "\n",
      "Translating 50 texts with Helsinki-NLP...\n",
      "Progress: 10/50 texts translated\n",
      "Progress: 20/50 texts translated\n",
      "Progress: 30/50 texts translated\n",
      "Progress: 40/50 texts translated\n",
      "Progress: 50/50 texts translated\n",
      "\n",
      "=== Example Translations from General Dataset ===\n",
      "Source: Hello, how are you?\n",
      "Reference: Bonjour, comment allez-vous?\n",
      "T5-small: Bonjour, comment êtes-vous ?\n",
      "T5-base: Bonjour, comment êtes-vous?\n",
      "Helsinki-NLP: Bonjour, comment allez-vous ?\n",
      "\n",
      "Section 1 complete! General dataset has been translated and stored in general_df.\n"
     ]
    }
   ],
   "source": [
    "# Section 1: General Topic Translations\n",
    "print(\"=== Section 1: General Topic Translations ===\")\n",
    "print(\"This section will be completed individually.\")\n",
    "\n",
    "# Function to translate a single text with a model\n",
    "def translate_text(text, model):\n",
    "    \"\"\"Translate a single text using the specified model.\"\"\"\n",
    "    return model(text)[0]['translation_text']\n",
    "\n",
    "# Function to translate a dataframe's source_text column\n",
    "def translate_dataframe(df, model, model_name, batch_size=10):\n",
    "    \"\"\"Translate all texts in a dataframe's source_text column.\"\"\"\n",
    "    translations = []\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    print(f\"\\nTranslating {total_rows} texts with {model_name}...\")\n",
    "    \n",
    "    # Process in batches to show progress\n",
    "    for i in range(0, total_rows, batch_size):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        for _, row in batch.iterrows():\n",
    "            translation = translate_text(row['source_text'], model, model_name)\n",
    "            translations.append(translation)\n",
    "        \n",
    "        # Show progress\n",
    "        progress = min(i + batch_size, total_rows)\n",
    "        print(f\"Progress: {progress}/{total_rows} texts translated\")\n",
    "    \n",
    "    return translations\n",
    "\n",
    "# Translate general dataset with each model\n",
    "print(\"\\nTranslating general dataset...\")\n",
    "general_translations = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTranslating with {model_name}...\")\n",
    "    translations = translate_dataframe(general_df, model, model_name)\n",
    "    general_translations[model_name] = translations\n",
    "    general_df[f'translation_{model_name.lower().replace(\"-\", \"_\")}'] = translations\n",
    "\n",
    "# Display examples from general dataset\n",
    "print(\"\\n=== Example Translations from General Dataset ===\")\n",
    "example = general_df.iloc[0]\n",
    "print(f\"Source: {example['source_text']}\")\n",
    "print(f\"Reference: {example['target_text']}\")\n",
    "for model_name in models.keys():\n",
    "    column_name = f'translation_{model_name.lower().replace(\"-\", \"_\")}'\n",
    "    print(f\"{model_name}: {example[column_name]}\")\n",
    "\n",
    "print(\"\\nSection 1 complete! General dataset has been translated and stored in general_df.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa2c4c9",
   "metadata": {},
   "source": [
    "## 4. Evaluating with BLEU Score\n",
    "\n",
    "Now that we have our translations, let's evaluate them using the BLEU score. We'll:\n",
    "1. Calculate BLEU scores for our test subset\n",
    "2. Calculate BLEU scores for the full dataset\n",
    "3. Compare scores across models and topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97344b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's evaluate our test subset\n",
    "print(\"Evaluating test subset with BLEU...\")\n",
    "test_bleu_scores = {}\n",
    "\n",
    "for model_name, model_translations in translations.items():\n",
    "    bleu_score = corpus_bleu(model_translations, [test_subset['target_text'].tolist()]).score\n",
    "    test_bleu_scores[model_name] = bleu_score\n",
    "    print(f\"{model_name} BLEU Score: {bleu_score:.2f}\")\n",
    "\n",
    "# Now evaluate the full dataset\n",
    "print(\"\\nEvaluating full dataset with BLEU...\")\n",
    "full_bleu_scores = {}\n",
    "\n",
    "for model_name, model_translations in full_translations.items():\n",
    "    bleu_score = corpus_bleu(model_translations, [all_data['target_text'].tolist()]).score\n",
    "    full_bleu_scores[model_name] = bleu_score\n",
    "    print(f\"{model_name} BLEU Score: {bleu_score:.2f}\")\n",
    "\n",
    "# Calculate BLEU scores by topic\n",
    "print(\"\\nCalculating BLEU scores by topic...\")\n",
    "topic_bleu_scores = {}\n",
    "\n",
    "for topic in ['general', 'politics', 'sports']:\n",
    "    topic_data = all_data[all_data['topic'] == topic]\n",
    "    topic_indices = topic_data.index\n",
    "    \n",
    "    print(f\"\\n{topic.capitalize()} Topic BLEU Scores:\")\n",
    "    for model_name, model_translations in full_translations.items():\n",
    "        topic_translations = [model_translations[i] for i in topic_indices]\n",
    "        bleu_score = corpus_bleu(topic_translations, [topic_data['target_text'].tolist()]).score\n",
    "        topic_bleu_scores[(model_name, topic)] = bleu_score\n",
    "        print(f\"{model_name}: {bleu_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24984e2",
   "metadata": {},
   "source": [
    "## 5. Evaluating with BERTScore\n",
    "\n",
    "Now let's evaluate our translations using BERTScore, which provides a different perspective on translation quality. We'll:\n",
    "1. Calculate BERTScore for our test subset\n",
    "2. Calculate BERTScore for the full dataset\n",
    "3. Compare scores across models and topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, evaluate our test subset\n",
    "print(\"Evaluating test subset with BERTScore...\")\n",
    "test_bert_scores = {}\n",
    "\n",
    "for model_name, model_translations in translations.items():\n",
    "    P, R, F1 = score(model_translations, test_subset['target_text'].tolist(), lang='fr', verbose=False)\n",
    "    bert_score = F1.mean().item()\n",
    "    test_bert_scores[model_name] = bert_score\n",
    "    print(f\"{model_name} BERTScore: {bert_score:.2f}\")\n",
    "\n",
    "# Now evaluate the full dataset\n",
    "print(\"\\nEvaluating full dataset with BERTScore...\")\n",
    "full_bert_scores = {}\n",
    "\n",
    "for model_name, model_translations in full_translations.items():\n",
    "    P, R, F1 = score(model_translations, all_data['target_text'].tolist(), lang='fr', verbose=False)\n",
    "    bert_score = F1.mean().item()\n",
    "    full_bert_scores[model_name] = bert_score\n",
    "    print(f\"{model_name} BERTScore: {bert_score:.2f}\")\n",
    "\n",
    "# Calculate BERTScore by topic\n",
    "print(\"\\nCalculating BERTScore by topic...\")\n",
    "topic_bert_scores = {}\n",
    "\n",
    "for topic in ['general', 'politics', 'sports']:\n",
    "    topic_data = all_data[all_data['topic'] == topic]\n",
    "    topic_indices = topic_data.index\n",
    "    \n",
    "    print(f\"\\n{topic.capitalize()} Topic BERTScore:\")\n",
    "    for model_name, model_translations in full_translations.items():\n",
    "        topic_translations = [model_translations[i] for i in topic_indices]\n",
    "        P, R, F1 = score(topic_translations, topic_data['target_text'].tolist(), lang='fr', verbose=False)\n",
    "        bert_score = F1.mean().item()\n",
    "        topic_bert_scores[(model_name, topic)] = bert_score\n",
    "        print(f\"{model_name}: {bert_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628cae2f",
   "metadata": {},
   "source": [
    "## 6. Combining and Analyzing Results\n",
    "\n",
    "Now let's combine all our evaluation results into a single DataFrame for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3595f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with all results\n",
    "results = []\n",
    "for model_name in models.keys():\n",
    "    for topic in ['general', 'politics', 'sports']:\n",
    "        results.append({\n",
    "            'model': model_name,\n",
    "            'topic': topic,\n",
    "            'bleu_score': topic_bleu_scores[(model_name, topic)],\n",
    "            'bert_score': topic_bert_scores[(model_name, topic)]\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(\"Evaluation Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f162cc9",
   "metadata": {},
   "source": [
    "## 7. Visualizing the Results\n",
    "\n",
    "Let's create visualizations to better understand how the models perform across different topics. We'll create:\n",
    "1. Bar plots comparing BLEU scores\n",
    "2. Bar plots comparing BERTScore\n",
    "3. A heatmap showing overall performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f58829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size for all plots\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "\n",
    "# 1. BLEU Scores by Model and Topic\n",
    "plt.figure()\n",
    "sns.barplot(data=results_df, x='model', y='bleu_score', hue='topic')\n",
    "plt.title('BLEU Scores by Model and Topic')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. BERTScore by Model and Topic\n",
    "plt.figure()\n",
    "sns.barplot(data=results_df, x='model', y='bert_score', hue='topic')\n",
    "plt.title('BERTScore by Model and Topic')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Heatmap of Model Performance\n",
    "pivot_bleu = results_df.pivot(index='model', columns='topic', values='bleu_score')\n",
    "plt.figure()\n",
    "sns.heatmap(pivot_bleu, annot=True, fmt='.2f', cmap='YlOrRd')\n",
    "plt.title('BLEU Score Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5deb3f0",
   "metadata": {},
   "source": [
    "## 8. Analyzing Topic-Specific Performance\n",
    "\n",
    "Let's analyze how each model performs on different topics. We'll look at:\n",
    "1. Best performing model for each topic\n",
    "2. Model rankings by both metrics\n",
    "3. Topic-specific strengths and weaknesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3078c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_topic_performance(topic):\n",
    "    topic_data = results_df[results_df['topic'] == topic]\n",
    "    \n",
    "    print(f\"\\n=== Performance Analysis for {topic.capitalize()} Topic ===\")\n",
    "    print(\"\\nBest Model by BLEU Score:\")\n",
    "    best_bleu = topic_data.loc[topic_data['bleu_score'].idxmax()]\n",
    "    print(f\"Model: {best_bleu['model']}, Score: {best_bleu['bleu_score']:.2f}\")\n",
    "    \n",
    "    print(\"\\nBest Model by BERTScore:\")\n",
    "    best_bert = topic_data.loc[topic_data['bert_score'].idxmax()]\n",
    "    print(f\"Model: {best_bert['model']}, Score: {best_bert['bert_score']:.2f}\")\n",
    "    \n",
    "    print(\"\\nModel Rankings:\")\n",
    "    print(\"BLEU Score Rankings:\")\n",
    "    print(topic_data.sort_values('bleu_score', ascending=False)[['model', 'bleu_score']])\n",
    "    print(\"\\nBERTScore Rankings:\")\n",
    "    print(topic_data.sort_values('bert_score', ascending=False)[['model', 'bert_score']])\n",
    "\n",
    "# Analyze each topic\n",
    "for topic in ['general', 'politics', 'sports']:\n",
    "    analyze_topic_performance(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deaf60d",
   "metadata": {},
   "source": [
    "## 9. Looking at Example Translations\n",
    "\n",
    "Finally, let's examine some actual translations from each model to qualitatively assess their performance. We'll look at:\n",
    "1. How well they handle topic-specific vocabulary\n",
    "2. The accuracy of their translations\n",
    "3. Any patterns in their strengths and weaknesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72177ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples(topic, num_examples=3):\n",
    "    topic_data = all_data[all_data['topic'] == topic]\n",
    "    examples = topic_data.sample(n=num_examples)\n",
    "    \n",
    "    print(f\"\\n=== Example Translations for {topic.capitalize()} Topic ===\")\n",
    "    for idx, row in examples.iterrows():\n",
    "        print(f\"\\nExample {idx + 1}:\")\n",
    "        print(f\"Source: {row['source_text']}\")\n",
    "        print(f\"Reference: {row['target_text']}\")\n",
    "        \n",
    "        # Get translations from each model\n",
    "        for model_name, model in models.items():\n",
    "            # Add task prefix for T5 models\n",
    "            if model_name.startswith('T5'):\n",
    "                text = f\"translate English to French: {row['source_text']}\"\n",
    "            else:\n",
    "                text = row['source_text']\n",
    "            \n",
    "            result = model(text)[0]['translation_text']\n",
    "            print(f\"{model_name}: {result}\")\n",
    "\n",
    "# Show examples for each topic\n",
    "for topic in ['general', 'politics', 'sports']:\n",
    "    show_examples(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e76d4c",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions\n",
    "\n",
    "In this notebook, we've:\n",
    "1. Loaded and prepared different types of translation data\n",
    "2. Set up three different translation models\n",
    "3. Translated the datasets\n",
    "4. Evaluated their performance using BLEU and BERTScore\n",
    "5. Visualized and analyzed the results\n",
    "6. Examined example translations\n",
    "\n",
    "Key takeaways:\n",
    "- Different models may perform better on different topics\n",
    "- Using multiple evaluation metrics gives us a more complete picture\n",
    "- The choice of model might depend on the specific use case\n",
    "- Topic-specific training data might improve performance\n",
    "\n",
    "Would you like to experiment with:\n",
    "1. Different models?\n",
    "2. Different topics?\n",
    "3. Different evaluation metrics?\n",
    "4. Different visualization approaches?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
